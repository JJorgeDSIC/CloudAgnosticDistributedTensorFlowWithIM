network publica (
  outbound = 'yes' and
  outports = '2222/tcp-2222/tcp,22/tcp-22/tcp,6006/tcp-6006/tcp,8888/tcp-8888/tcp, 9000/tcp-9000/tcp, 8033/tcp-8033/tcp' and
  provider_id = 'vpc-XXXXXXXX.subnet-2bfb6c4f'
)
network privada(
  outbound = 'no' and
  outports = '2222/tcp-2222/tcp,22/tcp-22/tcp,6006/tcp-6006/tcp, 9000/tcp-9000/tcp, 8033/tcp-8033/tcp, 8888/tcp-8888/tcp' and
  provider_id = 'vpc-XXXXXXXX.subnet-2bfb6c4f'
)


system ps (
  net_interface.0.connection = 'publica' and
  net_interface.0.dns_name = 'ps-0' and
  net_interface.1.connection = 'privada' and
  net_interface.1.dns_name = 'ps-0-priv' and
  disk.0.image.url = 'aws://us-east-1/ami-5c66ea23' and
  instance_type = 't2.micro' and
  disk.0.os.name='linux' and
  disk.0.os.credentials.username='ubuntu' and
  disk.0.applications contains (name='ansible.modules.git+https://github.com/JJorgeDSIC/ansible-role-hadoop-1|hadoop') and
  disk.0.applications contains (name='ansible.modules.git+https://github.com/JJorgeDSIC/ansible-role-tf|tf_node') and
  disk.0.os.credentials.public_key = 'USERKEYPAIR' and
  disk.0.os.credentials.private_key = '-----BEGIN RSA PRIVATE KEY-----
...
-----END RSA PRIVATE KEY-----'
)

system worker (
  net_interface.0.connection = 'privada' and
  net_interface.0.dns_name = 'worker-#N#' and
  net_interface.1.connection = 'publica' and
  disk.0.image.url = 'aws://us-east-1/ami-5c66ea23' and
  instance_type = 't2.micro' and
  disk.0.os.name='linux' and
  disk.0.os.credentials.username='ubuntu' and
  disk.0.applications contains (name='ansible.modules.git+https://github.com/JJorgeDSIC/ansible-role-hadoop-1|hadoop') and
  disk.0.applications contains (name='ansible.modules.git+https://github.com/JJorgeDSIC/ansible-role-tf|tf_node') and
  disk.0.os.credentials.public_key = 'USERKEYPAIR' and
  disk.0.os.credentials.private_key = '-----BEGIN RSA PRIVATE KEY-----
...
-----END RSA PRIVATE KEY-----'
)


configure ps (
@begin
---
  - vars:
      ps_nodes: 1
      worker_nodes: 2
      access_key_id: 'ACCESS_KEY'
      secret_access_key_id: 'SECRET_ACCESS_KEY'
    tasks:
      - lineinfile: dest=/etc/ssh/sshd_config regexp="PasswordAuthentication no" line="PasswordAuthentication no" state=absent
      - lineinfile: dest=/etc/ssh/sshd_config regexp="PasswordAuthentication yes" line="PasswordAuthentication yes" state=present

      - service: name=ssh state=restarted
      - name: Install openjdk-8-jdk
        apt: pkg=openjdk-8-jdk state=latest update_cache=yes

      - lineinfile: dest=/etc/bash.bashrc line="export HADOOP_HOME=/opt/hadoop" create=yes
      - lineinfile: dest=/etc/bash.bashrc line="export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop" create=yes
      - lineinfile: dest=/etc/bash.bashrc line="export PATH=$PATH:$HADOOP_HOME/bin:$SPARK_HOME/bin" create=yes
    roles:
      - { role: 'tf_node'}
      - { role: 'hadoop', hadoop_version: '2.9.0', hadoop_master: 'ps-0-priv', hadoop_type_of_node: 'master' }

@end
)

configure worker (
@begin
---
  - vars:
      ps_nodes: 1
      worker_nodes: 2
      access_key_id: 'ACCESS_KEY'
      secret_access_key_id: 'SECRET_ACCESS_KEY'
    tasks:
      - lineinfile: dest=/etc/ssh/sshd_config regexp="PasswordAuthentication no" line="PasswordAuthentication no" state=absent
      - lineinfile: dest=/etc/ssh/sshd_config regexp="PasswordAuthentication yes" line="PasswordAuthentication yes" state=present

      - service: name=ssh state=restarted
      - name: Install openjdk-8-jdk
        apt: pkg=openjdk-8-jdk state=latest update_cache=yes

      - lineinfile: dest=/etc/bash.bashrc line="export HADOOP_HOME=/opt/hadoop" create=yes
      - lineinfile: dest=/etc/bash.bashrc line="export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop" create=yes
      - lineinfile: dest=/etc/bash.bashrc line="export PATH=$PATH:$HADOOP_HOME/bin:$SPARK_HOME/bin" create=yes

    roles:
      - { role: 'tf_node'}
      - { role: 'hadoop', hadoop_version: '2.9.0', hadoop_master: 'ps-0-priv'}

@end
)


configure tf_hadoop_step1 (
@begin
---
  - vars:
      ps_nodes: 1
    tasks:
      - lineinfile: dest=/etc/bash.bashrc line="export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64" create=yes
      - lineinfile: dest=/etc/bash.bashrc line="export HADOOP_HDFS_HOME=/opt/hadoop-2.9.0" create=yes
@end
)

configure tf_hadoop_step2 (
@begin
---
  - vars:
      hadoop_home: /opt/hadoop-2.X
    tasks:
    - lineinfile: dest=/etc/bash.bashrc line="export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:${JAVA_HOME}/jre/lib/amd64/server" create=yes
    - lineinfile: dest=/etc/bash.bashrc line="export CLASSPATH=$(${HADOOP_HDFS_HOME}/bin/hadoop classpath --glob)" create=yes
@end
)

configure copying_data (
@begin
---
  - vars:
      hadoop_home: /opt/hadoop-2.X
      worker_nodes: 2
    tasks:
    # A small delay to let HDFS properly start. Required in ONE, not necessary in EC2
    # 90 is not enough
    - command: /bin/sleep 60
    - command: /opt/hadoop/bin/hadoop fs -mkdir {{ item }}
      with_items:
      - /mnist/
    - command: /opt/hadoop/bin/hadoop fs -mkdir {{ item }}
      with_items:
      - /models/
    - name: Creating training partitions
      command: /opt/hadoop/bin/hadoop fs -put /home/ubuntu/TensorFlowExamples/DATASETS/MNIST_LITE/mnist_train_lite.csv {{ item }}
      with_sequence: start=0 end={{ worker_nodes - 1 }} format=/mnist/part%d 
    - name: Creating test partitions
      command: /opt/hadoop/bin/hadoop fs -put /home/ubuntu/TensorFlowExamples/DATASETS/MNIST_LITE/mnist_test_lite.csv {{ item }}
      with_sequence: start=0 end={{ worker_nodes - 1 }} format=/mnist/test_part%d 
@end    
)


configure launch_training (
@begin
---
  - vars:
      hadoop_home: /opt/hadoop-2.X
    tasks:
    - command: chmod +x run.sh
      become: true
      args:
        chdir: /home/ubuntu/
    - name: Run the experiment.
      shell: /home/ubuntu/run.sh 2> /dev/null
      become: true
      args:
        chdir: /home/ubuntu/

@end    
)


configure upload_models (
@begin
---
  - vars:
  # Access to global variables heree?
      hadoop_home: /opt/hadoop-2.X
      worker_nodes: 2
    tasks:
    - command: /bin/sleep 60
    - name: Upload models
      command: /opt/hadoop/bin/hadoop fs -put /home/ubuntu/logs /models/$HOSTNAME
@end    
)


configure launch_tensorboard (
@begin
---
  - vars:
      hadoop_home: /opt/hadoop-2.X
    tasks:
    # A small delay 
    - command: /bin/sleep 60
    - command: tensorboard --logdir=/home/ubuntu/logs_summary/ --port=6006 
 
@end    
)

deploy ps 1
deploy worker 2

contextualize (
    system ps configure ps step 1
    system worker configure worker step 1
    system ps configure tf_hadoop_step1 step 2
    system worker configure tf_hadoop_step1 step 2
    system ps configure tf_hadoop_step2 step 3
    system worker configure tf_hadoop_step2 step 3
    system ps configure copying_data step 4
    system ps configure launch_training step 5
    system worker configure launch_training step 5
    system worker configure upload_models step 6
    system worker configure launch_tensorboard step 6
)
